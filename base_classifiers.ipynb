{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1: Imports and Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load the CSV data into a pandas DataFrame.\n",
    "    Expected columns: user_id, movie_id, rating, timestamp\n",
    "    \"\"\"\n",
    "    # df = pd.read_csv(filename, header=None, delimiter='\\t')\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    df.columns = ['user_id', 'movie_id', 'rating']\n",
    "    df['rating'] = df['rating'].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_data(filename):\n",
    "    \"\"\"\n",
    "    Load the CSV data into a pandas DataFrame.\n",
    "    Expected columns: user_id, movie_id, rating, timestamp\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Compute Global Stats and Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_movie_stats(df):\n",
    "    \"\"\"\n",
    "    Compute global average rating per movie and movie frequency.\n",
    "    \"\"\"\n",
    "    global_stats = df.groupby('movie_id')['rating'].agg(['mean', 'count']).rename(columns={'mean': 'global_mean', 'count': 'freq'})\n",
    "    return global_stats\n",
    "\n",
    "def compute_global_rating_distribution(df):\n",
    "    \"\"\"\n",
    "    Compute global rating distribution p(r=1), p(r=2), ..., p(r=5).\n",
    "    \"\"\"\n",
    "    rating_counts = df['rating'].value_counts()\n",
    "    total = rating_counts.sum()\n",
    "    dist = np.array([rating_counts.get(r, 0) for r in range(1,6)]) / total\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4: Build User Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_profiles(df):\n",
    "    \"\"\"\n",
    "    Build a dictionary of user -> {movie_id: rating}\n",
    "    \"\"\"\n",
    "    user_profiles = {}\n",
    "    for row in df.itertuples(index=False):\n",
    "        u = row.user_id\n",
    "        m = row.movie_id\n",
    "        r = row.rating\n",
    "        if u not in user_profiles:\n",
    "            user_profiles[u] = {}\n",
    "        user_profiles[u][m] = r\n",
    "    return user_profiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5: Feature Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RDAM(user_profile, global_stats):\n",
    "    \"\"\"\n",
    "    RDAM: Mean absolute deviation from global movie averages.\n",
    "    \"\"\"\n",
    "    diffs = []\n",
    "    for m, r in user_profile.items():\n",
    "        if m in global_stats.index:\n",
    "            diffs.append(abs(r - global_stats.loc[m, 'global_mean']))\n",
    "    return np.mean(diffs) if diffs else np.nan\n",
    "\n",
    "def calculate_WDA(user_profile, global_stats):\n",
    "    \"\"\"\n",
    "    WDA: Weighted deviation from average, weighted by movie frequency.\n",
    "    \"\"\"\n",
    "    weighted_diffs = []\n",
    "    freqs = []\n",
    "    for m, r in user_profile.items():\n",
    "        if m in global_stats.index:\n",
    "            diff = abs(r - global_stats.loc[m, 'global_mean'])\n",
    "            f = global_stats.loc[m, 'freq']\n",
    "            weighted_diffs.append(diff * f)\n",
    "            freqs.append(f)\n",
    "    if freqs:\n",
    "        return sum(weighted_diffs) / sum(freqs)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def calculate_cosine_similarity(user_profile, global_stats, all_movies):\n",
    "    \"\"\"\n",
    "    Cosine similarity between user's rating vector and global average rating vector.\n",
    "    \"\"\"\n",
    "    G = np.array([global_stats.loc[m, 'global_mean'] if m in global_stats.index else 0 for m in all_movies])\n",
    "    U = np.array([user_profile[m] if m in user_profile else 0 for m in all_movies])\n",
    "    if np.linalg.norm(U) == 0 or np.linalg.norm(G) == 0:\n",
    "        return np.nan\n",
    "    sim = (U.dot(G)) / (np.linalg.norm(U)*np.linalg.norm(G))\n",
    "    return sim\n",
    "\n",
    "def calculate_LenVar(user_profile):\n",
    "    \"\"\"\n",
    "    LenVar: Variance of user's ratings.\n",
    "    \"\"\"\n",
    "    ratings = list(user_profile.values())\n",
    "    if len(ratings) > 1:\n",
    "        return np.var(ratings)\n",
    "    else:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 6: TF-IDF Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_TFIDF(user_profiles, all_movies):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF for each user.\n",
    "    Treat each user as a 'document' and each movie as a 'term'.\n",
    "    \"\"\"\n",
    "    user_ids = sorted(user_profiles.keys())\n",
    "    # Build a binary matrix: rows = users, cols = movies\n",
    "    user_movie_matrix = np.zeros((len(user_ids), len(all_movies)))\n",
    "    movie_index_map = {m:i for i,m in enumerate(all_movies)}\n",
    "    \n",
    "    for ui, u in enumerate(user_ids):\n",
    "        for m, r in user_profiles[u].items():\n",
    "            user_movie_matrix[ui, movie_index_map[m]] = 1  # or use rating as TF\n",
    "    \n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf_matrix = transformer.fit_transform(user_movie_matrix)\n",
    "    # Average TF-IDF per user\n",
    "    avg_tfidf = tfidf_matrix.mean(axis=1).A1  # A1 converts sparse matrix to 1D array\n",
    "    \n",
    "    tfidf_dict = dict(zip(user_ids, avg_tfidf))\n",
    "    return tfidf_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 7: RDMA Similarity and RDMA_LenVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RDMA_similarity(user_profile, global_dist):\n",
    "    \"\"\"\n",
    "    RDMA similarity: Compare user's rating distribution to global rating distribution.\n",
    "    \"\"\"\n",
    "    ratings = list(user_profile.values())\n",
    "    if len(ratings) == 0:\n",
    "        return np.nan\n",
    "    user_dist = np.array([ratings.count(r) for r in range(1,6)]) / len(ratings)\n",
    "    mad = np.mean(np.abs(user_dist - global_dist))\n",
    "    return 1 - mad\n",
    "\n",
    "def calculate_RDMA_LenVar(rdma_similarity, lenvar):\n",
    "    \"\"\"\n",
    "    Combine RDMA similarity and LenVar.\n",
    "    RDMA_LenVar = RDMA_similarity * (1/(1+lenvar))\n",
    "    \"\"\"\n",
    "    if rdma_similarity is np.nan:\n",
    "        return np.nan\n",
    "    return rdma_similarity * (1/(1+lenvar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 8: Degree of Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_degree_of_similarity(user_profile, global_stats):\n",
    "    \"\"\"\n",
    "    Degree of similarity: correlation between user pattern (above/below movie mean) and \n",
    "    a global pattern.\n",
    "    \"\"\"\n",
    "    overall_avg_rating = global_stats['global_mean'].mean()\n",
    "    \n",
    "    user_movies = list(user_profile.keys())\n",
    "    if not user_movies:\n",
    "        return np.nan\n",
    "    \n",
    "    user_ratings = np.array([user_profile[m] for m in user_movies])\n",
    "    movie_means = np.array([global_stats.loc[m, 'global_mean'] if m in global_stats.index else overall_avg_rating \n",
    "                            for m in user_movies])\n",
    "    \n",
    "    U_bin = (user_ratings > movie_means).astype(int)\n",
    "    G_bin = (movie_means > overall_avg_rating).astype(int)\n",
    "    \n",
    "    if len(U_bin) > 1 and np.std(U_bin) > 0 and np.std(G_bin) > 0:\n",
    "        corr = np.corrcoef(U_bin, G_bin)[0,1]\n",
    "    else:\n",
    "        corr = 0.0\n",
    "    return corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 9: Main Execution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real User: Authenticity -> 1\n",
    "Fake User: Authenticity -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating\n",
      "0        1      7988       3\n",
      "1        1      5497       5\n",
      "2        1      7342       3\n",
      "3        1      5993       5\n",
      "4        1      8321       3\n"
     ]
    }
   ],
   "source": [
    "# filename = 'only_rating_attack_data.csv'  # Ensure this file is in the working directory or provide a full path\n",
    "# filename = './data/tool_home.csv'  \n",
    "filename = './data/grocery_food.csv'  \n",
    "df = load_data(filename)\n",
    "df['user_id'] = df['user_id'] + 1\n",
    "df['movie_id'] = df['movie_id'] + 1\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDAM: Rating Deviation from Average Mean\n",
    "WDA: Weighted Degree of Agreement\n",
    "LenVar: Length Variance\n",
    "RDMA_similarity: Rating Deviation Moving Average Similarity\n",
    "RDMA_LenVar: Rating Deviation Moving Average Length Variance\n",
    "degree_of_similarity: Degree of Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the filename if needed\n",
    "# filename = '../data/ml100k/real_data/real_data.csv'  # Ensure this file is in the working directory or provide a full path\n",
    "\n",
    "global_stats = compute_global_movie_stats(df)\n",
    "global_dist = compute_global_rating_distribution(df)\n",
    "user_profiles = build_user_profiles(df)\n",
    "all_movies = sorted(df['movie_id'].unique())\n",
    "\n",
    "# Precompute TF-IDF\n",
    "tfidf_dict = calculate_TFIDF(user_profiles, all_movies)\n",
    "\n",
    "user_features = []\n",
    "for u, profile in user_profiles.items():\n",
    "    rdam = calculate_RDAM(profile, global_stats)\n",
    "    wda = calculate_WDA(profile, global_stats)\n",
    "    cosim = calculate_cosine_similarity(profile, global_stats, all_movies)\n",
    "    lenvar = calculate_LenVar(profile)\n",
    "    user_tfidf = tfidf_dict[u]\n",
    "    rdma_sim = calculate_RDMA_similarity(profile, global_dist)\n",
    "    rdma_lenvar = calculate_RDMA_LenVar(rdma_sim, lenvar)\n",
    "    deg_sim = calculate_degree_of_similarity(profile, global_stats)\n",
    "\n",
    "    user_features.append({\n",
    "        'user_id': u,\n",
    "        'RDAM': rdam,\n",
    "        'WDA': wda,\n",
    "        'cosine_similarity': cosim,\n",
    "        'LenVar': lenvar,\n",
    "        'TF-IDF': user_tfidf,\n",
    "        'RDMA_similarity': rdma_sim,\n",
    "        'RDMA_LenVar': rdma_lenvar,\n",
    "        'degree_of_similarity': deg_sim\n",
    "    })\n",
    "\n",
    "features_df = pd.DataFrame(user_features)\n",
    "features_df['authenticity'] = features_df['user_id'].apply(lambda x: 0 if x > 1777 else 1)\n",
    "# features_df = features_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique user IDs: 1041\n",
      "Original unique IDs count: 1041\n"
     ]
    }
   ],
   "source": [
    "original_user_ids = features_df['user_id'].unique()\n",
    "\n",
    "# Step 2: Get attack and normal IDs\n",
    "attack_ids = features_df[features_df['user_id'] > 1777]['user_id'].values\n",
    "normal_ids = features_df[features_df['user_id'] <= 1777]['user_id'].values\n",
    "\n",
    "# Step 3: Randomly select normal IDs to swap with\n",
    "selected_normal_ids = np.random.choice(normal_ids, size=len(attack_ids), replace=False)\n",
    "\n",
    "# Step 4: Create a mapping dictionary for the swaps\n",
    "swap_dict = dict(zip(attack_ids, selected_normal_ids))\n",
    "swap_dict.update(dict(zip(selected_normal_ids, attack_ids)))\n",
    "\n",
    "# Step 5: Apply the swaps using the mapping\n",
    "features_df['user_id'] = features_df['user_id'].map(lambda x: swap_dict.get(x, x))\n",
    "features_df = features_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify the number of unique user IDs\n",
    "print(f\"Number of unique user IDs: {features_df['user_id'].nunique()}\")\n",
    "print(f\"Original unique IDs count: {len(original_user_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>RDAM</th>\n",
       "      <th>WDA</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>LenVar</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>RDMA_similarity</th>\n",
       "      <th>RDMA_LenVar</th>\n",
       "      <th>degree_of_similarity</th>\n",
       "      <th>authenticity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137</td>\n",
       "      <td>0.956893</td>\n",
       "      <td>0.930380</td>\n",
       "      <td>0.056725</td>\n",
       "      <td>0.035665</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.835826</td>\n",
       "      <td>0.807042</td>\n",
       "      <td>-0.460977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>629</td>\n",
       "      <td>0.636426</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.053842</td>\n",
       "      <td>0.853306</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.917378</td>\n",
       "      <td>0.494995</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185</td>\n",
       "      <td>0.659697</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.043303</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.858583</td>\n",
       "      <td>0.335967</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>0.408351</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.054864</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.814348</td>\n",
       "      <td>0.640798</td>\n",
       "      <td>0.043644</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>678</td>\n",
       "      <td>0.599834</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.049036</td>\n",
       "      <td>0.565097</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.912886</td>\n",
       "      <td>0.583278</td>\n",
       "      <td>-0.382047</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id      RDAM       WDA  cosine_similarity    LenVar    TF-IDF  \\\n",
       "0      137  0.956893  0.930380           0.056725  0.035665  0.000683   \n",
       "1      629  0.636426  0.755556           0.053842  0.853306  0.000615   \n",
       "2      185  0.659697  0.819444           0.043303  1.555556  0.000508   \n",
       "3       32  0.408351  0.384615           0.054864  0.270833  0.000639   \n",
       "4      678  0.599834  0.655556           0.049036  0.565097  0.000572   \n",
       "\n",
       "   RDMA_similarity  RDMA_LenVar  degree_of_similarity  authenticity  \n",
       "0         0.835826     0.807042             -0.460977             1  \n",
       "1         0.917378     0.494995             -0.266667             1  \n",
       "2         0.858583     0.335967              0.353553             1  \n",
       "3         0.814348     0.640798              0.043644             1  \n",
       "4         0.912886     0.583278             -0.382047             1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.to_csv('tool_home_attack_data.csv', index=False)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = './tool_home_attack_data.csv'\n",
    "new_df = load_new_data(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 1041\n",
      "Minimum value in column 0: 1\n",
      "Maximum value in column 0: 1041\n"
     ]
    }
   ],
   "source": [
    "# Understanding User Column (Column 0)\n",
    "unique_users = new_df['user_id'].unique()\n",
    "min_value = np.min(unique_users)\n",
    "max_value = np.max(unique_users)\n",
    "print('Number of unique users:', len(unique_users))\n",
    "print(f\"Minimum value in column 0: {min_value}\")\n",
    "print(f\"Maximum value in column 0: {max_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
